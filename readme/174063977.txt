H2O Driverless AI is Automatic Machine Learning for the Enterprise. Driverless AI automates feature engineering, model building, visualization and interpretability.
BYOR stands for Bring Your Own Recipe and is a key feature of Driverless AI. It allows domain scientists to solve their problems faster and with more precision.
Custom recipes are Python code snippets that can be uploaded into Driverless AI at runtime, like plugins. No need to restart Driverless AI. Custom recipes can be provided for transformers, models and scorers. During training of a supervised machine learning modeling pipeline (aka experiment), Driverless AI can then use these code snippets as building blocks, in combination with all built-in code pieces (or instead of). By providing your own custom recipes, you can gain control over the optimization choices that Driverless AI makes to best solve your machine learning problems.
Recipes are meant to be built by people you trust and each recipe should be code-reviewed before going to production.
Assume that a user with access to Driverless AI has access to the data inside that instance.
When recipes are enabled (enable_custom_recipes=true, the default), be aware that:
Best ways to control access to Driverless AI and custom recipes:
Go to Recipes for Driverless 1.7.0 1.7.1 1.8.0 1.8.1 1.8.2 1.8.3 1.8.4 1.8.5 1.8.6 1.8.7 1.8.8 1.9.0 1.9.1 1.9.2
Learn more about Driverless AI from the H2O.ai website
Take the test drive
Go to the Driverless AI community Slack channel and ask your BYOR related questions in #general
Recipes are meant to be built by people you trust and each recipe should be code-reviewed before going to production.
Assume that a user with access to Driverless AI has access to the data inside that instance.
Apart from securing access to the instance via private networks, various methods of authentication are possible. Local authentication provides the most control over which users have access to Driverless AI.
Unless the config.toml setting enable_dataset_downloading=false is set, an authenticated user can download all imported datasets as .csv via direct APIs.
When recipes are enabled (enable_custom_recipes=true, the default), be aware that:
The code for the recipes runs as the same native Linux user that runs the Driverless AI application.
Recipes have explicit access to all data passing through the transformer/model/scorer API
Recipes have implicit access to system resources such as disk, memory, CPUs, GPUs, network, etc.
A H2O-3 Java process is started in the background, for use by all recipes using H2O-3. Anyone with access to the Driverless AI instance can browse the file system, see models and data through the H2O-3 interface.
Best ways to control access to Driverless AI and custom recipes:
Control access to the Driverless AI instance
Use local authentication to specify exactly which users are allowed to access Driverless AI
Run Driverless AI in a Docker container, as a certain user, with only certain ports exposed, and only certain mount points mapped
To disable all recipes: Set enable_custom_recipes=false in the config.toml, or add the environment variable DRIVERLESS_AI_ENABLE_CUSTOM_RECIPES=0 at startup of Driverless AI. This will disable all custom transformers, models and scorers.
To disable new recipes: To keep all previously uploaded recipes enabled and disable the upload of any new recipes, set enable_custom_recipes_upload=false or DRIVERLESS_AI_ENABLE_CUSTOM_RECIPES_UPLOAD=0 at startup of Driverless AI.
Apart from securing access to the instance via private networks, various methods of authentication are possible. Local authentication provides the most control over which users have access to Driverless AI.
Unless the config.toml setting enable_dataset_downloading=false is set, an authenticated user can download all imported datasets as .csv via direct APIs.
The code for the recipes runs as the same native Linux user that runs the Driverless AI application.
Recipes have explicit access to all data passing through the transformer/model/scorer API
Recipes have implicit access to system resources such as disk, memory, CPUs, GPUs, network, etc.
A H2O-3 Java process is started in the background, for use by all recipes using H2O-3. Anyone with access to the Driverless AI instance can browse the file system, see models and data through the H2O-3 interface.
Recipes have explicit access to all data passing through the transformer/model/scorer API
Recipes have implicit access to system resources such as disk, memory, CPUs, GPUs, network, etc.
Control access to the Driverless AI instance
Use local authentication to specify exactly which users are allowed to access Driverless AI
Run Driverless AI in a Docker container, as a certain user, with only certain ports exposed, and only certain mount points mapped
To disable all recipes: Set enable_custom_recipes=false in the config.toml, or add the environment variable DRIVERLESS_AI_ENABLE_CUSTOM_RECIPES=0 at startup of Driverless AI. This will disable all custom transformers, models and scorers.
To disable new recipes: To keep all previously uploaded recipes enabled and disable the upload of any new recipes, set enable_custom_recipes_upload=false or DRIVERLESS_AI_ENABLE_CUSTOM_RECIPES_UPLOAD=0 at startup of Driverless AI.
Driverless AI automatically performs basic acceptance tests for all custom recipes unless disabled
More information in the FAQ
Use fast and efficient data manipulation tools like data.table, sklearn, numpy or pandas instead of Python lists, for-loops etc.
Use disk sparingly, delete temporary files as soon as possible
Use memory sparingly, delete objects when no longer needed
FAQ
Templates
Technical Architecture Diagram
AIR-GAPPED_INSTALLATIONS
load_custom_recipe.py [please add description]
DATA
GroupAgg.py [Aggregation features on numeric columns across multiple categorical columns]
KMeansClustering.py [Data Recipe to perform KMeans Clustering on a dataset.]
airlines.py [Create airlines dataset]
airlines_joined_data_flights_in_out.py [Create augmented airlines datasets]
airlines_joined_data_flights_in_out_regression.py [Create augmented airlines datasets for regression]
airlines_multiple.py [Create airlines dataset]
audio_to_image.py [Data recipe to transform input audio to Mel spectrogramsThis data recipe makes the following steps:1. Reads audio file2. Converts audio file to the Mel spectrogram3. Save Mel spectrogram to .png image4. Upload image dataset to DAIRecipe is based on the Kaggle Freesound Audio Tagging 2019 challenge:-audio-tagging-2019To use the recipe follow the next steps:1. Download a subsample of the audio dataset from here:-public-test-data.s3.amazonaws.com/bigdata/server/Image Data/freesound_audio.zip2. Unzip it and specify the path to the dataset in the DATA_DIR global variable3. Upload the dataset into Driverless AI using the Add Data Recipe optionThe transformed dataset is also available and could be directly uploaded to Driverless AI:-public-test-data.s3.amazonaws.com/bigdata/server/Image Data/freesound_images.zip]
bigquery_sharded_data_pull.py [Pull data >1Gb from GCP Bigquery using sharding functionality.Based on: -data#exporting_table_data #using_the_client_libraryAuthor: Travis CoutureCreated: 03/18/2020Last Updated: 03/18/2020]
catchallenge.py [Create cat challenge dataset]
covidtracking_daily_by_states.py [Upload daily Covid Tracking () US States cases, hospitalization, recovery, test and death data ]
create_transactional_data_or_convert_to_iid.py [Example code to generate and convert transactional data to i.i.d. data.]
creditcard.py [Modify credit card dataset]
data_template.py [Custom data recipe base class]
data_test.py [Create test dataset]
data_test_multiple.py [Create multiple dataset test]
feature_selection.py [please add description]
generate_random_int_columns.py [Data recipe to add one or more columns containing random integers.]
ieee_data_puddle.py [Data recipe to prepare data for Kaggle IEEE-CIS Fraud Detection -fraud-detection]
image_cropper.py [ Data Recipe to Crop the Cheque Image (or any Image) in a fixed dimension ]
imdb_datasets.py [Create IMDb datasets for main titles and episodes. Main titles includes info for both movies and series, while episodes contains series episodes details and ratings. Thus, there is one-to-many relationship between titles (one side) and episodes (many side which may be 0) Description: / Source: /]
kaggle_bosch.py [Create Bosch competition datasets with leak]
kaggle_ieee_fraud.py [Data recipe to prepare data for Kaggle IEEE-CIS Fraud Detection -fraud-detection]
kaggle_m5.py [Prepare data for m5 Kaggle Time-Series Forecast competition]
keywords_data.py [Check and match a list of words from a specific string column]
load_sas7bdat.py [Data Recipe to load a single sas file__version__ = 0.1authored by @mtanco (Michelle Tanco)Required User Defined Inputs: name of file to load]
mnist.py [Prep and upload the MNIST datasset]
mozilla_deepspeech_wav2txt.py [Speech to text using Mozilla's DeepSpeechSettings for this recipe:Assing MODEL_PATH global variable prior to usageAssign WAV_COLNAME global variable with proper column name from your dataset.This colums should contain absolute paths to .wav file which needs to be converted to text.General requirements to .wav's:1 channel (mono)16 bit16000 frequency]
nytimes_covid19_cases_deaths_by_counties.py [Upload daily COVID-19 cases and deaths in US by counties - NY Times github Source: nytimes/covid-19-data Coronavirus (Covid-19) Data in the United States -19-data]
nytimes_covid19_cases_deaths_by_states.py [Upload daily COVID-19 cases and deaths in US by states from NY Times github]
nytimes_covid19_cases_deaths_us.py [Upload daily COVID-19 cases and deaths in US total from NY Times github]
owid_covid19_cases_deaths_by_countries.py [Upload daily COVID-19 cases and deaths by countries Source: Our World in Data. It is updated daily and includes data on confirmed cases, deaths, and testing. -source-data]
seattle_rain_modify.py [Transpose the Monthly Seattle Rain Inches data set for Time Series use cases]
seattle_rain_upload.py [Upload Monthly Seattle Rain Inches data set from data provided by the City of Seattle]
ts_fill_n_cluster.py [Data Recipe to fill missing values in TS data and then create new data sets from TS Clustering]
two_sigma_rental.py [please add description]
video_to_image.py [Data recipe to transform input video to the images.This data recipe makes the following steps:1. Reads video file2. Samples N uniform frames from the video file3. Detects all faces on each frame4. Crops the faces and saves them as imagesRecipe is based on the Kaggle Deepfake Detection Challenge:-detection-challengeTo use the recipe follow the next steps:1. Download a small subsample of the video dataset from here:-public-test-data.s3.amazonaws.com/bigdata/server/Image Data/deepfake.zip2. Unzip it and specify the path to the dataset in the DATA_DIR global variable3. Upload the dataset into Driverless AI using the Add Data Recipe optionThe transformed dataset is also available and could be directly uploaded to Driverless AI:-public-test-data.s3.amazonaws.com/bigdata/server/Image Data/deepfake_frames.zip]
wav2txt.py [Speech to text using Azure Cognitive ServicesSettings for this recipe:Assing AZURE_SERVICE_KEY and AZURE_SERVICE_REGION global variable prior to usageAssign WAV_COLNAME global variable with proper column name from your dataset.This colums should contain absolute paths to .wav file which needs to be converted to text.]
DATABASES
create_dataset_from_mongodb_collection.py [Create dataset from MonogDB]
NLP
sentiment_score.py [Data recipe to get sentiment score using textblob]
sentiment_score_vader.py [Data recipe to get sentiment score using vader]
text_summarization.py [Data recipe to get summary of text using gensim]
tokenize_chinese.py [Chinese text tokenization using jieba package - ]
topic_modeling.py [Data recipe to perform topic modeling]
twitter_preprocessing_recipe.py [Preprocess the tweets by normalising username, removing unnecessary punctuations, expanding the hashtags]
EXPLAINERS
DOC
IMAGES
EXPLAINERS
morris_sensitivity_explainer.py [Morris Sensitivity Analysis Explainer]
TEMPLATES
template_dt_explainer.py [Decision Tree explainer which can be used to create explainer with global and local decision tree explanations.]
template_featimp_explainer.py [Feature importance explainer template which can be used create explainer with global and local feature importance explanations.]
template_md_explainer.py [Markdown report with raster image chart explainer template which can be used to create explainer with global report explanations.]
template_md_featimp_summary_explainer.py [Markdown report with summary feature importance chart explainer template which can be used to create explainer with global report explanations.]
template_md_vega_explainer.py [Markdown report with Vega chart explainer template which can be used to create explainer which creates global report explanations.]
template_pd_explainer.py [PD and ICE explainer template which can be used to create example with partial dependence (global) and individual conditional explanations (local) explanations.]
template_scatter_plot_explainer.py [Scatter plot explainer template which can be used to create explainer with global and local explanations.]
NOTEBOOKS
HOW_TO_WRITE_A_RECIPE
ExampleLogTransformer.py [please add description]
MODELS
mock_gpu_test_model.py [For GPU usage testing purposes.]
model_template.py [Template base class for a custom model recipe.]
ALGORITHMS
calibratedClassifier.py [ Calibrated Classifier Model: To calibrate predictions using Platt's scaling, Isotonic Regression or Splines] ✓ MOJO Enabled
catboost.py [CatBoost gradient boosting by Yandex. Currently supports regression and binary classification.]
daal_trees.py [Binary Classification and Regression for Decision Forest and Gradient Boosting based on Intel DAAL]
extra_trees.py [Extremely Randomized Trees (ExtraTrees) model from sklearn]
extremeClassifier.py [ Extreme Classifier Model: To speed up train of multiclass model (100s of classes) for lightGBM. Caution: can only be used for AUC (or GINI) and accuracy metrics. Based on: Extreme Classification in Log Memory using Count-Min Sketch: ]
h2o-3-gbm-poisson.py [H2O-3 Distributed Scalable Machine Learning Models: Poisson GBM]
h2o-3-models.py [H2O-3 Distributed Scalable Machine Learning Models (DL/GLM/GBM/DRF/NB/AutoML)]
h2o-glm-poisson.py [H2O-3 Distributed Scalable Machine Learning Models: Poisson GLM]
knearestneighbour.py [K-Nearest Neighbor implementation by sklearn. For small data (< 200k rows).]
libfm_fastfm.py [LibFM implementation of fastFM ]
linear_svm.py [Linear Support Vector Machine (SVM) implementation by sklearn. For small data.]
logistic_regression.py [Logistic Regression based upon sklearn.]
nusvm.py [Nu-SVM implementation by sklearn. For small data.]
random_forest.py [Random Forest (RandomForest) model from sklearn]
CUSTOM_LOSS
lightgbm_quantile_regression.py [Modified version of Driverless AI's internal LightGBM implementation with for quantile regression] ✓ MOJO Enabled
lightgbm_tweedie.py [Modified version of Driverless AI's internal LightGBM implementation with tweedie distribution] ✓ MOJO Enabled
lightgbm_with_custom_loss.py [Modified version of Driverless AI's internal LightGBM implementation with a custom objective function (used for tree split finding).] ✓ MOJO Enabled
xgboost_with_custom_loss.py [Modified version of Driverless AI's internal XGBoost implementation with a custom objective function (used for tree split finding).] ✓ MOJO Enabled
MLI
model_decision_tree_linear_combo.py [Decision tree plus linear model]
model_gam.py [Generalized Additive Model]
model_skopes_rules.py [Skopes rules ]
FAIRNESS
model_fairxgb.py [Fair XGB ]
reweighing_recipe.py [Debiasing using reweighing]
GA2M
model_ga2m.py [Explainable Boosting Machines (EBM), implementation of GA2M]
XNN
model_xnn.py [ Explainable neural net ]
MONOTONIC
monotonic_models.py [LightGBM/XGBoostGBM/DecisionTree with user-given monotonicity constraints (1/-1/0) for original numeric features]
NLP
finbert.py [Custom Bert model which uses FinBert pretrained weights. Can easily be adapted to other pretrained models, like SciBert.] ✓ MOJO Enabled
portuguese_bert.py [Custom Bert model pretrained on Portuguese.] ✓ MOJO Enabled
text_binary_count_logistic.py [Text classification model using binary count of words]
text_tfidf_model.py [Text classification / regression model using TFIDF]
text_tfidf_model_continuous.py [Text classification model using TFIDF]
TIMESERIES
exponential_smoothing.py [Linear Model on top of Exponential Weighted Moving Average Lags for Time-Series. Provide appropriate lags and past outcomes during batch scoring for best results.]
fb_prophet.py [Prophet by Facebook for TimeSeries with an example of parameter mutation.]
fb_prophet_parallel.py [Prophet by Facebook for TimeSeries with an example of parameter mutation.]
historic_mean.py [Historic Mean for Time-Series problems. Predicts the mean of the target for each timegroup for regression problems.]
UPLIFT
GLMTransformedOutcome.py [GLM for uplift modeling] ✓ MOJO Enabled
LightGBMTransformedOutcome.py [LightGBM for uplift modeling] ✓ MOJO Enabled
XGBoostTransformedOutcome.py [XGBoost for uplift modeling] ✓ MOJO Enabled
RECIPES
amazon.py [Recipe for Kaggle Competition: Amazon.com - Employee Access Challenge]
REFERENCE
SCORERS
huber_loss.py [Huber Loss for Regression or Binary Classification. Robust loss, combination of quadratic loss and linear loss.]
scorer_template.py [Template base class for a custom scorer recipe.]
CLASSIFICATION
f3_score.py [F3 Score]
f4_score.py [F4 Score]
precision.py [Weighted Precision: TP / (TP + FP) at threshold for optimal F1 Score.]
recall.py [Weighted Recall: TP / (TP + FN) at threshold for optimal F1 Score.]
BINARY
average_mcc.py [Averaged Matthews Correlation Coefficient (averaged over several thresholds, for imbalanced problems). Example how to use Driverless AI's internal scorer.]
brier_loss.py [Brier Loss]
cost.py [Using hard-coded dollar amounts x for false positives and y for false negatives, calculate the cost of a model using: (x * FP + y * FN) / N]
cost_access_to_data.py [Same as CostBinary, but provides access to full Data]
cost_smooth.py [Using hard-coded dollar amounts x for false positives and y for false negatives, calculate the cost of a model using: (1 - y_true) * y_pred * fp_cost + y_true * (1 - y_pred) * fn_cost]
false_discovery_rate.py [Weighted False Discovery Rate: FP / (FP + TP) at threshold for optimal F1 Score.]
logloss_with_costs.py [Logloss with costs associated with each type of 4 outcomes - typically applicable to fraud use case]
marketing_campaign.py [Computes the mean profit per outbound marketing letter, given a fraction of the population addressed, and fixed cost and reward]
profit.py [Uses domain information about user behavior to calculate the profit or loss of a model.]
MULTICLASS
hamming_loss.py [Hamming Loss - Misclassification Rate (1 - Accuracy)]
map@k.py [Mean Average Precision @ k (MAP@k)]
quadratic_weighted_kappa.py [Qudratic Weighted Kappa]
REGRESSION
WAPE_scorer.py [Weighted Absoluted Percent Error]
asymmetric_mae.py [MAE with a penalty that differs for positive and negative errors]
auuc.py [Area under uplift curve]
cosh_loss.py [Hyperbolic Cosine Loss]
explained_variance.py [Explained Variance. Fraction of variance that is explained by the model.]
largest_error.py [Largest error for regression problems. Highly sensitive to outliers.]
log_mae.py [Log Mean Absolute Error for regression]
mean_absolute_scaled_error.py [Mean Absolute Scaled Error for time-series regression]
mean_squared_log_error.py [Mean Squared Log Error for regression]
median_absolute_error.py [Median Absolute Error for regression]
pearson_correlation.py [Pearson Correlation Coefficient for regression]
quantile_loss.py [Quantile Loss regression]
r2_by_tgc.py [Custom R2 scorer computes R2 on each time series, then averages them out for the final score.]
rmse_with_x.py [Custom RMSE Scorer that also gets X (original features) - for demo/testing purposes only]
top_decile.py [Median Absolute Error for predictions in the top decile]
TRANSFORMERS
how_to_debug_transformer.py [Example how to debug a transformer outside of Driverless AI (optional)]
how_to_test_from_py_client.py [Testing a BYOR Transformer the PyClient - works on 1.7.0 & 1.7.1-17]
transformer_template.py [Template base class for a custom transformer recipe.]
AUGMENTATION
germany_landers_holidays.py [Returns a flag for whether a date falls on a holiday for each of Germany's Bundeslaender. ]
ipaddress_features.py [Parses IP addresses and networks and extracts its properties.]
is_ramadan.py [Returns a flag for whether a date falls on Ramadan in Saudi Arabia]
singapore_public_holidays.py [Flag for whether a date falls on a public holiday in Singapore.]
usairportcode_origin_dest.py [Transformer to parse and augment US airport codes with geolocation info.]
usairportcode_origin_dest_geo_features.py [Transformer to augment US airport codes with geolocation info.]
uszipcode_features_database.py [Transformer to parse and augment US zipcodes with info from zipcode database.]
uszipcode_features_light.py [Lightweight transformer to parse and augment US zipcodes with info from zipcode database.]
DATETIME
datetime_diff_transformer.py [Difference in time between two datetime columns]
datetime_encoder_transformer.py [Converts datetime column into an integer (milliseconds since 1970)]
days_until_dec2020.py [Creates new feature for any date columns, by computing the difference in days between the date value and 31st Dec 2020]
EXECUTABLES
pe_data_directory_features.py [Extract LIEF features from PE files]
pe_exports_features.py [Extract LIEF features from PE files]
pe_general_features.py [Extract LIEF features from PE files]
pe_header_features.py [Extract LIEF features from PE files]
pe_imports_features.py [Extract LIEF features from PE files]
pe_normalized_byte_count.py [Extract LIEF features from PE files]
pe_section_characteristics.py [Extract LIEF features from PE files]
DATA
GENERIC
count_missing_values_transformer.py [Count of missing values per row]
missing_flag_transformer.py [Returns 1 if a value is missing, or 0 otherwise]
specific_column_transformer.py [Example of a transformer that operates on the entire original frame, and hence on any column(s) desired.]
GEOSPATIAL
geodesic.py [Calculates the distance in miles between two latitude/longitude points in space]
myhaversine.py [Computes miles between first two *_latitude and *_longitude named columns in the data set]
HIERARCHICAL
firstNCharCVTE.py [Target-encode high cardinality categorical text by their first few characters in the string ]
log_scale_target_encoding.py [Target-encode numbers by their logarithm]
IMAGE
image_ocr_transformer.py [Convert a path to an image to text using OCR based on tesseract]
image_url_transformer.py [Convert a path to an image (JPG/JPEG/PNG) to a vector of class probabilities created by a pretrained ImageNet deeplearning model (Keras, TensorFlow).]
NLP
continuous_TextTransformer.py [please add description] ✓ MOJO Enabled
fuzzy_text_similarity_transformers.py [Row-by-row similarity between two text columns based on FuzzyWuzzy]
text_binary_count_transformer.py [Explainable Text transformer that uses binary counts of words using sklearn's CountVectorizer]
text_char_tfidf_count_transformers.py [Character level TFIDF and Count followed by Truncated SVD on text columns]
text_embedding_similarity_transformers.py [Row-by-row similarity between two text columns based on pretrained Deep Learning embedding space]
text_lang_detect_transformer.py [Detect the language for a text value using Google's 'langdetect' package]
text_meta_transformers.py [Extract common meta features from text]
text_named_entities_transformer.py [Extract the counts of different named entities in the text (e.g. Person, Organization, Location)]
text_pos_tagging_transformer.py [Extract the count of nouns, verbs, adjectives and adverbs in the text]
text_preprocessing_transformer.py [Preprocess the text column by stemming, lemmatization and stop word removal]
text_readability_transformers.py [ Custom Recipe to extract Readability features from the text data]
text_sentiment_transformer.py [Extract sentiment from text using pretrained models from TextBlob]
text_similarity_transformers.py [Row-by-row similarity between two text columns based on common N-grams, Jaccard similarity, Dice similarity and edit distance.]
text_spelling_correction_transformers.py [Correct the spelling of text column]
text_topic_modeling_transformer.py [Extract topics from text column using LDA]
text_url_summary_transformer.py [Extract text from URL and summarizes it]
vader_text_sentiment_transformer.py [Extract sentiment from text using lexicon and rule-based sentiment analysis tool called VADER]
NUMERIC
boxcox_transformer.py [Box-Cox Transform]
count_negative_values_transformer.py [Count of negative values per row]
count_positive_values_transformer.py [Count of positive values per row]
exp_diff_transformer.py [Exponentiated difference of two numbers]
log_transformer.py [Converts numbers to their Logarithm] ✓ MOJO Enabled
product.py [Products together 3 or more numeric features]
random_transformer.py [Creates random numbers]
round_transformer.py [Rounds numbers to 1, 2 or 3 decimals] ✓ MOJO Enabled
square_root_transformer.py [Converts numbers to the square root, preserving the sign of the original numbers]
sum.py [Adds together 3 or more numeric features]
truncated_svd_all.py [Truncated SVD for all columns]
yeojohnson_transformer.py [Yeo-Johnson Power Transformer]
OUTLIERS
h2o3-dl-anomaly.py [Anomaly score for each row based on reconstruction error of a H2O-3 deep learning autoencoder]
quantile_winsorizer.py [Winsorizes (truncates) univariate outliers outside of a given quantile threshold]
twosigma_winsorizer.py [Winsorizes (truncates) univariate outliers outside of two standard deviations from the mean.]
RECOMMENDATIONS
matrixfactorization.py [Collaborative filtering features using various techniques of Matrix Factorization for recommendations.Recommended for large data]
SIGNAL_PROCESSING
signal_processing.py [This custom transformer processes signal files to create features used by DriverlessAI to solve a regression problem]
SPEECH
audio_MFCC_transformer.py [Extract MFCC and spectrogram features from audio files]
azure_speech_to_text.py [An example of integration with Azure Speech Recognition Service]
STRING
simple_grok_parser.py [Extract column data using grok patterns]
strlen_transformer.py [Returns the string length of categorical values]
to_string_transformer.py [Converts numbers to strings]
user_agent_transformer.py [A best effort transformer to determine browser device characteristics from a user-agent string]
SURVIVAL
dummy-pretransformer.py [Dummy Pre-Transformer to use as a template for custom pre-transformer recipes. This transformer consumes all features at once, adds 'pre:' to the names and passes them down to transformer level and GA as-is.]
h2o-3-coxph-pretransformer.py [Pre-transformer utilizing survival analysis modeling using CoxPH (Cox proportional hazard) using H2O-3 CoxPH function. It adds risk score produced by CoxPH model and drops stop_column feature used for survival modeling along with actual target as event.]
TARGETENCODING
ExpandingMean.py [CatBoost-style target encoding. See  for short explanation]
leaky_mean_target_encoder.py [Example implementation of a out-of-fold target encoder (leaky, not recommended)]
TIMESERIES
auto_arima_forecast.py [Auto ARIMA transformer is a time series transformer that predicts target using ARIMA models.]
general_time_series_transformer.py [Demonstrates the API for custom time-series transformers.]
parallel_auto_arima_forecast.py [Parallel Auto ARIMA transformer is a time series transformer that predicts target using ARIMA models.In this implementation, Time Group Models are fitted in parallel]
parallel_prophet_forecast.py [Parallel FB Prophet transformer is a time series transformer that predicts target using FBProphet models.]
parallel_prophet_forecast_using_individual_groups.py [Parallel FB Prophet transformer is a time series transformer that predicts target using FBProphet models.This transformer fits one model for each time group column values and is significantly fasterthan the implementation available in parallel_prophet_forecast.py.]
serial_prophet_forecast.py [Transformer that uses FB Prophet for time series prediction.Please see the parallel implementation for more information]
time_encoder_transformer.py [converts the Time Column to an ordered integer]
trading_volatility.py [Calculates Historical Volatility for numeric features (makes assumptions on the data)]
load_custom_recipe.py [please add description]
GroupAgg.py [Aggregation features on numeric columns across multiple categorical columns]
KMeansClustering.py [Data Recipe to perform KMeans Clustering on a dataset.]
airlines.py [Create airlines dataset]
airlines_joined_data_flights_in_out.py [Create augmented airlines datasets]
airlines_joined_data_flights_in_out_regression.py [Create augmented airlines datasets for regression]
airlines_multiple.py [Create airlines dataset]
audio_to_image.py [Data recipe to transform input audio to Mel spectrogramsThis data recipe makes the following steps:1. Reads audio file2. Converts audio file to the Mel spectrogram3. Save Mel spectrogram to .png image4. Upload image dataset to DAIRecipe is based on the Kaggle Freesound Audio Tagging 2019 challenge:-audio-tagging-2019To use the recipe follow the next steps:1. Download a subsample of the audio dataset from here:-public-test-data.s3.amazonaws.com/bigdata/server/Image Data/freesound_audio.zip2. Unzip it and specify the path to the dataset in the DATA_DIR global variable3. Upload the dataset into Driverless AI using the Add Data Recipe optionThe transformed dataset is also available and could be directly uploaded to Driverless AI:-public-test-data.s3.amazonaws.com/bigdata/server/Image Data/freesound_images.zip]
bigquery_sharded_data_pull.py [Pull data >1Gb from GCP Bigquery using sharding functionality.Based on: -data#exporting_table_data #using_the_client_libraryAuthor: Travis CoutureCreated: 03/18/2020Last Updated: 03/18/2020]
catchallenge.py [Create cat challenge dataset]
covidtracking_daily_by_states.py [Upload daily Covid Tracking () US States cases, hospitalization, recovery, test and death data ]
create_transactional_data_or_convert_to_iid.py [Example code to generate and convert transactional data to i.i.d. data.]
creditcard.py [Modify credit card dataset]
data_template.py [Custom data recipe base class]
data_test.py [Create test dataset]
data_test_multiple.py [Create multiple dataset test]
feature_selection.py [please add description]
generate_random_int_columns.py [Data recipe to add one or more columns containing random integers.]
ieee_data_puddle.py [Data recipe to prepare data for Kaggle IEEE-CIS Fraud Detection -fraud-detection]
image_cropper.py [ Data Recipe to Crop the Cheque Image (or any Image) in a fixed dimension ]
imdb_datasets.py [Create IMDb datasets for main titles and episodes. Main titles includes info for both movies and series, while episodes contains series episodes details and ratings. Thus, there is one-to-many relationship between titles (one side) and episodes (many side which may be 0) Description: / Source: /]
kaggle_bosch.py [Create Bosch competition datasets with leak]
kaggle_ieee_fraud.py [Data recipe to prepare data for Kaggle IEEE-CIS Fraud Detection -fraud-detection]
kaggle_m5.py [Prepare data for m5 Kaggle Time-Series Forecast competition]
keywords_data.py [Check and match a list of words from a specific string column]
load_sas7bdat.py [Data Recipe to load a single sas file__version__ = 0.1authored by @mtanco (Michelle Tanco)Required User Defined Inputs: name of file to load]
mnist.py [Prep and upload the MNIST datasset]
mozilla_deepspeech_wav2txt.py [Speech to text using Mozilla's DeepSpeechSettings for this recipe:Assing MODEL_PATH global variable prior to usageAssign WAV_COLNAME global variable with proper column name from your dataset.This colums should contain absolute paths to .wav file which needs to be converted to text.General requirements to .wav's:1 channel (mono)16 bit16000 frequency]
nytimes_covid19_cases_deaths_by_counties.py [Upload daily COVID-19 cases and deaths in US by counties - NY Times github Source: nytimes/covid-19-data Coronavirus (Covid-19) Data in the United States -19-data]
nytimes_covid19_cases_deaths_by_states.py [Upload daily COVID-19 cases and deaths in US by states from NY Times github]
nytimes_covid19_cases_deaths_us.py [Upload daily COVID-19 cases and deaths in US total from NY Times github]
owid_covid19_cases_deaths_by_countries.py [Upload daily COVID-19 cases and deaths by countries Source: Our World in Data. It is updated daily and includes data on confirmed cases, deaths, and testing. -source-data]
seattle_rain_modify.py [Transpose the Monthly Seattle Rain Inches data set for Time Series use cases]
seattle_rain_upload.py [Upload Monthly Seattle Rain Inches data set from data provided by the City of Seattle]
ts_fill_n_cluster.py [Data Recipe to fill missing values in TS data and then create new data sets from TS Clustering]
two_sigma_rental.py [please add description]
video_to_image.py [Data recipe to transform input video to the images.This data recipe makes the following steps:1. Reads video file2. Samples N uniform frames from the video file3. Detects all faces on each frame4. Crops the faces and saves them as imagesRecipe is based on the Kaggle Deepfake Detection Challenge:-detection-challengeTo use the recipe follow the next steps:1. Download a small subsample of the video dataset from here:-public-test-data.s3.amazonaws.com/bigdata/server/Image Data/deepfake.zip2. Unzip it and specify the path to the dataset in the DATA_DIR global variable3. Upload the dataset into Driverless AI using the Add Data Recipe optionThe transformed dataset is also available and could be directly uploaded to Driverless AI:-public-test-data.s3.amazonaws.com/bigdata/server/Image Data/deepfake_frames.zip]
wav2txt.py [Speech to text using Azure Cognitive ServicesSettings for this recipe:Assing AZURE_SERVICE_KEY and AZURE_SERVICE_REGION global variable prior to usageAssign WAV_COLNAME global variable with proper column name from your dataset.This colums should contain absolute paths to .wav file which needs to be converted to text.]
DATABASES
create_dataset_from_mongodb_collection.py [Create dataset from MonogDB]
NLP
sentiment_score.py [Data recipe to get sentiment score using textblob]
sentiment_score_vader.py [Data recipe to get sentiment score using vader]
text_summarization.py [Data recipe to get summary of text using gensim]
tokenize_chinese.py [Chinese text tokenization using jieba package - ]
topic_modeling.py [Data recipe to perform topic modeling]
twitter_preprocessing_recipe.py [Preprocess the tweets by normalising username, removing unnecessary punctuations, expanding the hashtags]
create_dataset_from_mongodb_collection.py [Create dataset from MonogDB]
sentiment_score.py [Data recipe to get sentiment score using textblob]
sentiment_score_vader.py [Data recipe to get sentiment score using vader]
text_summarization.py [Data recipe to get summary of text using gensim]
tokenize_chinese.py [Chinese text tokenization using jieba package - ]
topic_modeling.py [Data recipe to perform topic modeling]
twitter_preprocessing_recipe.py [Preprocess the tweets by normalising username, removing unnecessary punctuations, expanding the hashtags]
DOC
IMAGES
EXPLAINERS
morris_sensitivity_explainer.py [Morris Sensitivity Analysis Explainer]
TEMPLATES
template_dt_explainer.py [Decision Tree explainer which can be used to create explainer with global and local decision tree explanations.]
template_featimp_explainer.py [Feature importance explainer template which can be used create explainer with global and local feature importance explanations.]
template_md_explainer.py [Markdown report with raster image chart explainer template which can be used to create explainer with global report explanations.]
template_md_featimp_summary_explainer.py [Markdown report with summary feature importance chart explainer template which can be used to create explainer with global report explanations.]
template_md_vega_explainer.py [Markdown report with Vega chart explainer template which can be used to create explainer which creates global report explanations.]
template_pd_explainer.py [PD and ICE explainer template which can be used to create example with partial dependence (global) and individual conditional explanations (local) explanations.]
template_scatter_plot_explainer.py [Scatter plot explainer template which can be used to create explainer with global and local explanations.]
NOTEBOOKS
IMAGES
morris_sensitivity_explainer.py [Morris Sensitivity Analysis Explainer]
TEMPLATES
template_dt_explainer.py [Decision Tree explainer which can be used to create explainer with global and local decision tree explanations.]
template_featimp_explainer.py [Feature importance explainer template which can be used create explainer with global and local feature importance explanations.]
template_md_explainer.py [Markdown report with raster image chart explainer template which can be used to create explainer with global report explanations.]
template_md_featimp_summary_explainer.py [Markdown report with summary feature importance chart explainer template which can be used to create explainer with global report explanations.]
template_md_vega_explainer.py [Markdown report with Vega chart explainer template which can be used to create explainer which creates global report explanations.]
template_pd_explainer.py [PD and ICE explainer template which can be used to create example with partial dependence (global) and individual conditional explanations (local) explanations.]
template_scatter_plot_explainer.py [Scatter plot explainer template which can be used to create explainer with global and local explanations.]
template_dt_explainer.py [Decision Tree explainer which can be used to create explainer with global and local decision tree explanations.]
template_featimp_explainer.py [Feature importance explainer template which can be used create explainer with global and local feature importance explanations.]
template_md_explainer.py [Markdown report with raster image chart explainer template which can be used to create explainer with global report explanations.]
template_md_featimp_summary_explainer.py [Markdown report with summary feature importance chart explainer template which can be used to create explainer with global report explanations.]
template_md_vega_explainer.py [Markdown report with Vega chart explainer template which can be used to create explainer which creates global report explanations.]
template_pd_explainer.py [PD and ICE explainer template which can be used to create example with partial dependence (global) and individual conditional explanations (local) explanations.]
template_scatter_plot_explainer.py [Scatter plot explainer template which can be used to create explainer with global and local explanations.]
ExampleLogTransformer.py [please add description]
mock_gpu_test_model.py [For GPU usage testing purposes.]
model_template.py [Template base class for a custom model recipe.]
ALGORITHMS
calibratedClassifier.py [ Calibrated Classifier Model: To calibrate predictions using Platt's scaling, Isotonic Regression or Splines] ✓ MOJO Enabled
catboost.py [CatBoost gradient boosting by Yandex. Currently supports regression and binary classification.]
daal_trees.py [Binary Classification and Regression for Decision Forest and Gradient Boosting based on Intel DAAL]
extra_trees.py [Extremely Randomized Trees (ExtraTrees) model from sklearn]
extremeClassifier.py [ Extreme Classifier Model: To speed up train of multiclass model (100s of classes) for lightGBM. Caution: can only be used for AUC (or GINI) and accuracy metrics. Based on: Extreme Classification in Log Memory using Count-Min Sketch: ]
h2o-3-gbm-poisson.py [H2O-3 Distributed Scalable Machine Learning Models: Poisson GBM]
h2o-3-models.py [H2O-3 Distributed Scalable Machine Learning Models (DL/GLM/GBM/DRF/NB/AutoML)]
h2o-glm-poisson.py [H2O-3 Distributed Scalable Machine Learning Models: Poisson GLM]
knearestneighbour.py [K-Nearest Neighbor implementation by sklearn. For small data (< 200k rows).]
libfm_fastfm.py [LibFM implementation of fastFM ]
linear_svm.py [Linear Support Vector Machine (SVM) implementation by sklearn. For small data.]
logistic_regression.py [Logistic Regression based upon sklearn.]
nusvm.py [Nu-SVM implementation by sklearn. For small data.]
random_forest.py [Random Forest (RandomForest) model from sklearn]
CUSTOM_LOSS
lightgbm_quantile_regression.py [Modified version of Driverless AI's internal LightGBM implementation with for quantile regression] ✓ MOJO Enabled
lightgbm_tweedie.py [Modified version of Driverless AI's internal LightGBM implementation with tweedie distribution] ✓ MOJO Enabled
lightgbm_with_custom_loss.py [Modified version of Driverless AI's internal LightGBM implementation with a custom objective function (used for tree split finding).] ✓ MOJO Enabled
xgboost_with_custom_loss.py [Modified version of Driverless AI's internal XGBoost implementation with a custom objective function (used for tree split finding).] ✓ MOJO Enabled
MLI
model_decision_tree_linear_combo.py [Decision tree plus linear model]
model_gam.py [Generalized Additive Model]
model_skopes_rules.py [Skopes rules ]
FAIRNESS
model_fairxgb.py [Fair XGB ]
reweighing_recipe.py [Debiasing using reweighing]
GA2M
model_ga2m.py [Explainable Boosting Machines (EBM), implementation of GA2M]
XNN
model_xnn.py [ Explainable neural net ]
MONOTONIC
monotonic_models.py [LightGBM/XGBoostGBM/DecisionTree with user-given monotonicity constraints (1/-1/0) for original numeric features]
NLP
finbert.py [Custom Bert model which uses FinBert pretrained weights. Can easily be adapted to other pretrained models, like SciBert.] ✓ MOJO Enabled
portuguese_bert.py [Custom Bert model pretrained on Portuguese.] ✓ MOJO Enabled
text_binary_count_logistic.py [Text classification model using binary count of words]
text_tfidf_model.py [Text classification / regression model using TFIDF]
text_tfidf_model_continuous.py [Text classification model using TFIDF]
TIMESERIES
exponential_smoothing.py [Linear Model on top of Exponential Weighted Moving Average Lags for Time-Series. Provide appropriate lags and past outcomes during batch scoring for best results.]
fb_prophet.py [Prophet by Facebook for TimeSeries with an example of parameter mutation.]
fb_prophet_parallel.py [Prophet by Facebook for TimeSeries with an example of parameter mutation.]
historic_mean.py [Historic Mean for Time-Series problems. Predicts the mean of the target for each timegroup for regression problems.]
UPLIFT
GLMTransformedOutcome.py [GLM for uplift modeling] ✓ MOJO Enabled
LightGBMTransformedOutcome.py [LightGBM for uplift modeling] ✓ MOJO Enabled
XGBoostTransformedOutcome.py [XGBoost for uplift modeling] ✓ MOJO Enabled
calibratedClassifier.py [ Calibrated Classifier Model: To calibrate predictions using Platt's scaling, Isotonic Regression or Splines] ✓ MOJO Enabled
catboost.py [CatBoost gradient boosting by Yandex. Currently supports regression and binary classification.]
daal_trees.py [Binary Classification and Regression for Decision Forest and Gradient Boosting based on Intel DAAL]
extra_trees.py [Extremely Randomized Trees (ExtraTrees) model from sklearn]
extremeClassifier.py [ Extreme Classifier Model: To speed up train of multiclass model (100s of classes) for lightGBM. Caution: can only be used for AUC (or GINI) and accuracy metrics. Based on: Extreme Classification in Log Memory using Count-Min Sketch: ]
h2o-3-gbm-poisson.py [H2O-3 Distributed Scalable Machine Learning Models: Poisson GBM]
h2o-3-models.py [H2O-3 Distributed Scalable Machine Learning Models (DL/GLM/GBM/DRF/NB/AutoML)]
h2o-glm-poisson.py [H2O-3 Distributed Scalable Machine Learning Models: Poisson GLM]
knearestneighbour.py [K-Nearest Neighbor implementation by sklearn. For small data (< 200k rows).]
libfm_fastfm.py [LibFM implementation of fastFM ]
linear_svm.py [Linear Support Vector Machine (SVM) implementation by sklearn. For small data.]
logistic_regression.py [Logistic Regression based upon sklearn.]
nusvm.py [Nu-SVM implementation by sklearn. For small data.]
random_forest.py [Random Forest (RandomForest) model from sklearn]
lightgbm_quantile_regression.py [Modified version of Driverless AI's internal LightGBM implementation with for quantile regression] ✓ MOJO Enabled
lightgbm_tweedie.py [Modified version of Driverless AI's internal LightGBM implementation with tweedie distribution] ✓ MOJO Enabled
lightgbm_with_custom_loss.py [Modified version of Driverless AI's internal LightGBM implementation with a custom objective function (used for tree split finding).] ✓ MOJO Enabled
xgboost_with_custom_loss.py [Modified version of Driverless AI's internal XGBoost implementation with a custom objective function (used for tree split finding).] ✓ MOJO Enabled
model_decision_tree_linear_combo.py [Decision tree plus linear model]
model_gam.py [Generalized Additive Model]
model_skopes_rules.py [Skopes rules ]
FAIRNESS
model_fairxgb.py [Fair XGB ]
reweighing_recipe.py [Debiasing using reweighing]
GA2M
model_ga2m.py [Explainable Boosting Machines (EBM), implementation of GA2M]
XNN
model_xnn.py [ Explainable neural net ]
model_fairxgb.py [Fair XGB ]
reweighing_recipe.py [Debiasing using reweighing]
model_ga2m.py [Explainable Boosting Machines (EBM), implementation of GA2M]
model_xnn.py [ Explainable neural net ]
monotonic_models.py [LightGBM/XGBoostGBM/DecisionTree with user-given monotonicity constraints (1/-1/0) for original numeric features]
finbert.py [Custom Bert model which uses FinBert pretrained weights. Can easily be adapted to other pretrained models, like SciBert.] ✓ MOJO Enabled
portuguese_bert.py [Custom Bert model pretrained on Portuguese.] ✓ MOJO Enabled
text_binary_count_logistic.py [Text classification model using binary count of words]
text_tfidf_model.py [Text classification / regression model using TFIDF]
text_tfidf_model_continuous.py [Text classification model using TFIDF]
exponential_smoothing.py [Linear Model on top of Exponential Weighted Moving Average Lags for Time-Series. Provide appropriate lags and past outcomes during batch scoring for best results.]
fb_prophet.py [Prophet by Facebook for TimeSeries with an example of parameter mutation.]
fb_prophet_parallel.py [Prophet by Facebook for TimeSeries with an example of parameter mutation.]
historic_mean.py [Historic Mean for Time-Series problems. Predicts the mean of the target for each timegroup for regression problems.]
GLMTransformedOutcome.py [GLM for uplift modeling] ✓ MOJO Enabled
LightGBMTransformedOutcome.py [LightGBM for uplift modeling] ✓ MOJO Enabled
XGBoostTransformedOutcome.py [XGBoost for uplift modeling] ✓ MOJO Enabled
amazon.py [Recipe for Kaggle Competition: Amazon.com - Employee Access Challenge]
huber_loss.py [Huber Loss for Regression or Binary Classification. Robust loss, combination of quadratic loss and linear loss.]
scorer_template.py [Template base class for a custom scorer recipe.]
CLASSIFICATION
f3_score.py [F3 Score]
f4_score.py [F4 Score]
precision.py [Weighted Precision: TP / (TP + FP) at threshold for optimal F1 Score.]
recall.py [Weighted Recall: TP / (TP + FN) at threshold for optimal F1 Score.]
BINARY
average_mcc.py [Averaged Matthews Correlation Coefficient (averaged over several thresholds, for imbalanced problems). Example how to use Driverless AI's internal scorer.]
brier_loss.py [Brier Loss]
cost.py [Using hard-coded dollar amounts x for false positives and y for false negatives, calculate the cost of a model using: (x * FP + y * FN) / N]
cost_access_to_data.py [Same as CostBinary, but provides access to full Data]
cost_smooth.py [Using hard-coded dollar amounts x for false positives and y for false negatives, calculate the cost of a model using: (1 - y_true) * y_pred * fp_cost + y_true * (1 - y_pred) * fn_cost]
false_discovery_rate.py [Weighted False Discovery Rate: FP / (FP + TP) at threshold for optimal F1 Score.]
logloss_with_costs.py [Logloss with costs associated with each type of 4 outcomes - typically applicable to fraud use case]
marketing_campaign.py [Computes the mean profit per outbound marketing letter, given a fraction of the population addressed, and fixed cost and reward]
profit.py [Uses domain information about user behavior to calculate the profit or loss of a model.]
MULTICLASS
hamming_loss.py [Hamming Loss - Misclassification Rate (1 - Accuracy)]
map@k.py [Mean Average Precision @ k (MAP@k)]
quadratic_weighted_kappa.py [Qudratic Weighted Kappa]
REGRESSION
WAPE_scorer.py [Weighted Absoluted Percent Error]
asymmetric_mae.py [MAE with a penalty that differs for positive and negative errors]
auuc.py [Area under uplift curve]
cosh_loss.py [Hyperbolic Cosine Loss]
explained_variance.py [Explained Variance. Fraction of variance that is explained by the model.]
largest_error.py [Largest error for regression problems. Highly sensitive to outliers.]
log_mae.py [Log Mean Absolute Error for regression]
mean_absolute_scaled_error.py [Mean Absolute Scaled Error for time-series regression]
mean_squared_log_error.py [Mean Squared Log Error for regression]
median_absolute_error.py [Median Absolute Error for regression]
pearson_correlation.py [Pearson Correlation Coefficient for regression]
quantile_loss.py [Quantile Loss regression]
r2_by_tgc.py [Custom R2 scorer computes R2 on each time series, then averages them out for the final score.]
rmse_with_x.py [Custom RMSE Scorer that also gets X (original features) - for demo/testing purposes only]
top_decile.py [Median Absolute Error for predictions in the top decile]
f3_score.py [F3 Score]
f4_score.py [F4 Score]
precision.py [Weighted Precision: TP / (TP + FP) at threshold for optimal F1 Score.]
recall.py [Weighted Recall: TP / (TP + FN) at threshold for optimal F1 Score.]
BINARY
average_mcc.py [Averaged Matthews Correlation Coefficient (averaged over several thresholds, for imbalanced problems). Example how to use Driverless AI's internal scorer.]
brier_loss.py [Brier Loss]
cost.py [Using hard-coded dollar amounts x for false positives and y for false negatives, calculate the cost of a model using: (x * FP + y * FN) / N]
cost_access_to_data.py [Same as CostBinary, but provides access to full Data]
cost_smooth.py [Using hard-coded dollar amounts x for false positives and y for false negatives, calculate the cost of a model using: (1 - y_true) * y_pred * fp_cost + y_true * (1 - y_pred) * fn_cost]
false_discovery_rate.py [Weighted False Discovery Rate: FP / (FP + TP) at threshold for optimal F1 Score.]
logloss_with_costs.py [Logloss with costs associated with each type of 4 outcomes - typically applicable to fraud use case]
marketing_campaign.py [Computes the mean profit per outbound marketing letter, given a fraction of the population addressed, and fixed cost and reward]
profit.py [Uses domain information about user behavior to calculate the profit or loss of a model.]
MULTICLASS
hamming_loss.py [Hamming Loss - Misclassification Rate (1 - Accuracy)]
map@k.py [Mean Average Precision @ k (MAP@k)]
quadratic_weighted_kappa.py [Qudratic Weighted Kappa]
average_mcc.py [Averaged Matthews Correlation Coefficient (averaged over several thresholds, for imbalanced problems). Example how to use Driverless AI's internal scorer.]
brier_loss.py [Brier Loss]
cost.py [Using hard-coded dollar amounts x for false positives and y for false negatives, calculate the cost of a model using: (x * FP + y * FN) / N]
cost_access_to_data.py [Same as CostBinary, but provides access to full Data]
cost_smooth.py [Using hard-coded dollar amounts x for false positives and y for false negatives, calculate the cost of a model using: (1 - y_true) * y_pred * fp_cost + y_true * (1 - y_pred) * fn_cost]
false_discovery_rate.py [Weighted False Discovery Rate: FP / (FP + TP) at threshold for optimal F1 Score.]
logloss_with_costs.py [Logloss with costs associated with each type of 4 outcomes - typically applicable to fraud use case]
marketing_campaign.py [Computes the mean profit per outbound marketing letter, given a fraction of the population addressed, and fixed cost and reward]
profit.py [Uses domain information about user behavior to calculate the profit or loss of a model.]
hamming_loss.py [Hamming Loss - Misclassification Rate (1 - Accuracy)]
map@k.py [Mean Average Precision @ k (MAP@k)]
quadratic_weighted_kappa.py [Qudratic Weighted Kappa]
WAPE_scorer.py [Weighted Absoluted Percent Error]
asymmetric_mae.py [MAE with a penalty that differs for positive and negative errors]
auuc.py [Area under uplift curve]
cosh_loss.py [Hyperbolic Cosine Loss]
explained_variance.py [Explained Variance. Fraction of variance that is explained by the model.]
largest_error.py [Largest error for regression problems. Highly sensitive to outliers.]
log_mae.py [Log Mean Absolute Error for regression]
mean_absolute_scaled_error.py [Mean Absolute Scaled Error for time-series regression]
mean_squared_log_error.py [Mean Squared Log Error for regression]
median_absolute_error.py [Median Absolute Error for regression]
pearson_correlation.py [Pearson Correlation Coefficient for regression]
quantile_loss.py [Quantile Loss regression]
r2_by_tgc.py [Custom R2 scorer computes R2 on each time series, then averages them out for the final score.]
rmse_with_x.py [Custom RMSE Scorer that also gets X (original features) - for demo/testing purposes only]
top_decile.py [Median Absolute Error for predictions in the top decile]
how_to_debug_transformer.py [Example how to debug a transformer outside of Driverless AI (optional)]
how_to_test_from_py_client.py [Testing a BYOR Transformer the PyClient - works on 1.7.0 & 1.7.1-17]
transformer_template.py [Template base class for a custom transformer recipe.]
AUGMENTATION
germany_landers_holidays.py [Returns a flag for whether a date falls on a holiday for each of Germany's Bundeslaender. ]
ipaddress_features.py [Parses IP addresses and networks and extracts its properties.]
is_ramadan.py [Returns a flag for whether a date falls on Ramadan in Saudi Arabia]
singapore_public_holidays.py [Flag for whether a date falls on a public holiday in Singapore.]
usairportcode_origin_dest.py [Transformer to parse and augment US airport codes with geolocation info.]
usairportcode_origin_dest_geo_features.py [Transformer to augment US airport codes with geolocation info.]
uszipcode_features_database.py [Transformer to parse and augment US zipcodes with info from zipcode database.]
uszipcode_features_light.py [Lightweight transformer to parse and augment US zipcodes with info from zipcode database.]
DATETIME
datetime_diff_transformer.py [Difference in time between two datetime columns]
datetime_encoder_transformer.py [Converts datetime column into an integer (milliseconds since 1970)]
days_until_dec2020.py [Creates new feature for any date columns, by computing the difference in days between the date value and 31st Dec 2020]
EXECUTABLES
pe_data_directory_features.py [Extract LIEF features from PE files]
pe_exports_features.py [Extract LIEF features from PE files]
pe_general_features.py [Extract LIEF features from PE files]
pe_header_features.py [Extract LIEF features from PE files]
pe_imports_features.py [Extract LIEF features from PE files]
pe_normalized_byte_count.py [Extract LIEF features from PE files]
pe_section_characteristics.py [Extract LIEF features from PE files]
DATA
GENERIC
count_missing_values_transformer.py [Count of missing values per row]
missing_flag_transformer.py [Returns 1 if a value is missing, or 0 otherwise]
specific_column_transformer.py [Example of a transformer that operates on the entire original frame, and hence on any column(s) desired.]
GEOSPATIAL
geodesic.py [Calculates the distance in miles between two latitude/longitude points in space]
myhaversine.py [Computes miles between first two *_latitude and *_longitude named columns in the data set]
HIERARCHICAL
firstNCharCVTE.py [Target-encode high cardinality categorical text by their first few characters in the string ]
log_scale_target_encoding.py [Target-encode numbers by their logarithm]
IMAGE
image_ocr_transformer.py [Convert a path to an image to text using OCR based on tesseract]
image_url_transformer.py [Convert a path to an image (JPG/JPEG/PNG) to a vector of class probabilities created by a pretrained ImageNet deeplearning model (Keras, TensorFlow).]
NLP
continuous_TextTransformer.py [please add description] ✓ MOJO Enabled
fuzzy_text_similarity_transformers.py [Row-by-row similarity between two text columns based on FuzzyWuzzy]
text_binary_count_transformer.py [Explainable Text transformer that uses binary counts of words using sklearn's CountVectorizer]
text_char_tfidf_count_transformers.py [Character level TFIDF and Count followed by Truncated SVD on text columns]
text_embedding_similarity_transformers.py [Row-by-row similarity between two text columns based on pretrained Deep Learning embedding space]
text_lang_detect_transformer.py [Detect the language for a text value using Google's 'langdetect' package]
text_meta_transformers.py [Extract common meta features from text]
text_named_entities_transformer.py [Extract the counts of different named entities in the text (e.g. Person, Organization, Location)]
text_pos_tagging_transformer.py [Extract the count of nouns, verbs, adjectives and adverbs in the text]
text_preprocessing_transformer.py [Preprocess the text column by stemming, lemmatization and stop word removal]
text_readability_transformers.py [ Custom Recipe to extract Readability features from the text data]
text_sentiment_transformer.py [Extract sentiment from text using pretrained models from TextBlob]
text_similarity_transformers.py [Row-by-row similarity between two text columns based on common N-grams, Jaccard similarity, Dice similarity and edit distance.]
text_spelling_correction_transformers.py [Correct the spelling of text column]
text_topic_modeling_transformer.py [Extract topics from text column using LDA]
text_url_summary_transformer.py [Extract text from URL and summarizes it]
vader_text_sentiment_transformer.py [Extract sentiment from text using lexicon and rule-based sentiment analysis tool called VADER]
NUMERIC
boxcox_transformer.py [Box-Cox Transform]
count_negative_values_transformer.py [Count of negative values per row]
count_positive_values_transformer.py [Count of positive values per row]
exp_diff_transformer.py [Exponentiated difference of two numbers]
log_transformer.py [Converts numbers to their Logarithm] ✓ MOJO Enabled
product.py [Products together 3 or more numeric features]
random_transformer.py [Creates random numbers]
round_transformer.py [Rounds numbers to 1, 2 or 3 decimals] ✓ MOJO Enabled
square_root_transformer.py [Converts numbers to the square root, preserving the sign of the original numbers]
sum.py [Adds together 3 or more numeric features]
truncated_svd_all.py [Truncated SVD for all columns]
yeojohnson_transformer.py [Yeo-Johnson Power Transformer]
OUTLIERS
h2o3-dl-anomaly.py [Anomaly score for each row based on reconstruction error of a H2O-3 deep learning autoencoder]
quantile_winsorizer.py [Winsorizes (truncates) univariate outliers outside of a given quantile threshold]
twosigma_winsorizer.py [Winsorizes (truncates) univariate outliers outside of two standard deviations from the mean.]
RECOMMENDATIONS
matrixfactorization.py [Collaborative filtering features using various techniques of Matrix Factorization for recommendations.Recommended for large data]
SIGNAL_PROCESSING
signal_processing.py [This custom transformer processes signal files to create features used by DriverlessAI to solve a regression problem]
SPEECH
audio_MFCC_transformer.py [Extract MFCC and spectrogram features from audio files]
azure_speech_to_text.py [An example of integration with Azure Speech Recognition Service]
STRING
simple_grok_parser.py [Extract column data using grok patterns]
strlen_transformer.py [Returns the string length of categorical values]
to_string_transformer.py [Converts numbers to strings]
user_agent_transformer.py [A best effort transformer to determine browser device characteristics from a user-agent string]
SURVIVAL
dummy-pretransformer.py [Dummy Pre-Transformer to use as a template for custom pre-transformer recipes. This transformer consumes all features at once, adds 'pre:' to the names and passes them down to transformer level and GA as-is.]
h2o-3-coxph-pretransformer.py [Pre-transformer utilizing survival analysis modeling using CoxPH (Cox proportional hazard) using H2O-3 CoxPH function. It adds risk score produced by CoxPH model and drops stop_column feature used for survival modeling along with actual target as event.]
TARGETENCODING
ExpandingMean.py [CatBoost-style target encoding. See  for short explanation]
leaky_mean_target_encoder.py [Example implementation of a out-of-fold target encoder (leaky, not recommended)]
TIMESERIES
auto_arima_forecast.py [Auto ARIMA transformer is a time series transformer that predicts target using ARIMA models.]
general_time_series_transformer.py [Demonstrates the API for custom time-series transformers.]
parallel_auto_arima_forecast.py [Parallel Auto ARIMA transformer is a time series transformer that predicts target using ARIMA models.In this implementation, Time Group Models are fitted in parallel]
parallel_prophet_forecast.py [Parallel FB Prophet transformer is a time series transformer that predicts target using FBProphet models.]
parallel_prophet_forecast_using_individual_groups.py [Parallel FB Prophet transformer is a time series transformer that predicts target using FBProphet models.This transformer fits one model for each time group column values and is significantly fasterthan the implementation available in parallel_prophet_forecast.py.]
serial_prophet_forecast.py [Transformer that uses FB Prophet for time series prediction.Please see the parallel implementation for more information]
time_encoder_transformer.py [converts the Time Column to an ordered integer]
trading_volatility.py [Calculates Historical Volatility for numeric features (makes assumptions on the data)]
germany_landers_holidays.py [Returns a flag for whether a date falls on a holiday for each of Germany's Bundeslaender. ]
ipaddress_features.py [Parses IP addresses and networks and extracts its properties.]
is_ramadan.py [Returns a flag for whether a date falls on Ramadan in Saudi Arabia]
singapore_public_holidays.py [Flag for whether a date falls on a public holiday in Singapore.]
usairportcode_origin_dest.py [Transformer to parse and augment US airport codes with geolocation info.]
usairportcode_origin_dest_geo_features.py [Transformer to augment US airport codes with geolocation info.]
uszipcode_features_database.py [Transformer to parse and augment US zipcodes with info from zipcode database.]
uszipcode_features_light.py [Lightweight transformer to parse and augment US zipcodes with info from zipcode database.]
datetime_diff_transformer.py [Difference in time between two datetime columns]
datetime_encoder_transformer.py [Converts datetime column into an integer (milliseconds since 1970)]
days_until_dec2020.py [Creates new feature for any date columns, by computing the difference in days between the date value and 31st Dec 2020]
pe_data_directory_features.py [Extract LIEF features from PE files]
pe_exports_features.py [Extract LIEF features from PE files]
pe_general_features.py [Extract LIEF features from PE files]
pe_header_features.py [Extract LIEF features from PE files]
pe_imports_features.py [Extract LIEF features from PE files]
pe_normalized_byte_count.py [Extract LIEF features from PE files]
pe_section_characteristics.py [Extract LIEF features from PE files]
DATA
count_missing_values_transformer.py [Count of missing values per row]
missing_flag_transformer.py [Returns 1 if a value is missing, or 0 otherwise]
specific_column_transformer.py [Example of a transformer that operates on the entire original frame, and hence on any column(s) desired.]
geodesic.py [Calculates the distance in miles between two latitude/longitude points in space]
myhaversine.py [Computes miles between first two *_latitude and *_longitude named columns in the data set]
firstNCharCVTE.py [Target-encode high cardinality categorical text by their first few characters in the string ]
log_scale_target_encoding.py [Target-encode numbers by their logarithm]
image_ocr_transformer.py [Convert a path to an image to text using OCR based on tesseract]
image_url_transformer.py [Convert a path to an image (JPG/JPEG/PNG) to a vector of class probabilities created by a pretrained ImageNet deeplearning model (Keras, TensorFlow).]
continuous_TextTransformer.py [please add description] ✓ MOJO Enabled
fuzzy_text_similarity_transformers.py [Row-by-row similarity between two text columns based on FuzzyWuzzy]
text_binary_count_transformer.py [Explainable Text transformer that uses binary counts of words using sklearn's CountVectorizer]
text_char_tfidf_count_transformers.py [Character level TFIDF and Count followed by Truncated SVD on text columns]
text_embedding_similarity_transformers.py [Row-by-row similarity between two text columns based on pretrained Deep Learning embedding space]
text_lang_detect_transformer.py [Detect the language for a text value using Google's 'langdetect' package]
text_meta_transformers.py [Extract common meta features from text]
text_named_entities_transformer.py [Extract the counts of different named entities in the text (e.g. Person, Organization, Location)]
text_pos_tagging_transformer.py [Extract the count of nouns, verbs, adjectives and adverbs in the text]
text_preprocessing_transformer.py [Preprocess the text column by stemming, lemmatization and stop word removal]
text_readability_transformers.py [ Custom Recipe to extract Readability features from the text data]
text_sentiment_transformer.py [Extract sentiment from text using pretrained models from TextBlob]
text_similarity_transformers.py [Row-by-row similarity between two text columns based on common N-grams, Jaccard similarity, Dice similarity and edit distance.]
text_spelling_correction_transformers.py [Correct the spelling of text column]
text_topic_modeling_transformer.py [Extract topics from text column using LDA]
text_url_summary_transformer.py [Extract text from URL and summarizes it]
vader_text_sentiment_transformer.py [Extract sentiment from text using lexicon and rule-based sentiment analysis tool called VADER]
boxcox_transformer.py [Box-Cox Transform]
count_negative_values_transformer.py [Count of negative values per row]
count_positive_values_transformer.py [Count of positive values per row]
exp_diff_transformer.py [Exponentiated difference of two numbers]
log_transformer.py [Converts numbers to their Logarithm] ✓ MOJO Enabled
product.py [Products together 3 or more numeric features]
random_transformer.py [Creates random numbers]
round_transformer.py [Rounds numbers to 1, 2 or 3 decimals] ✓ MOJO Enabled
square_root_transformer.py [Converts numbers to the square root, preserving the sign of the original numbers]
sum.py [Adds together 3 or more numeric features]
truncated_svd_all.py [Truncated SVD for all columns]
yeojohnson_transformer.py [Yeo-Johnson Power Transformer]
h2o3-dl-anomaly.py [Anomaly score for each row based on reconstruction error of a H2O-3 deep learning autoencoder]
quantile_winsorizer.py [Winsorizes (truncates) univariate outliers outside of a given quantile threshold]
twosigma_winsorizer.py [Winsorizes (truncates) univariate outliers outside of two standard deviations from the mean.]
matrixfactorization.py [Collaborative filtering features using various techniques of Matrix Factorization for recommendations.Recommended for large data]
signal_processing.py [This custom transformer processes signal files to create features used by DriverlessAI to solve a regression problem]
audio_MFCC_transformer.py [Extract MFCC and spectrogram features from audio files]
azure_speech_to_text.py [An example of integration with Azure Speech Recognition Service]
simple_grok_parser.py [Extract column data using grok patterns]
strlen_transformer.py [Returns the string length of categorical values]
to_string_transformer.py [Converts numbers to strings]
user_agent_transformer.py [A best effort transformer to determine browser device characteristics from a user-agent string]
dummy-pretransformer.py [Dummy Pre-Transformer to use as a template for custom pre-transformer recipes. This transformer consumes all features at once, adds 'pre:' to the names and passes them down to transformer level and GA as-is.]
h2o-3-coxph-pretransformer.py [Pre-transformer utilizing survival analysis modeling using CoxPH (Cox proportional hazard) using H2O-3 CoxPH function. It adds risk score produced by CoxPH model and drops stop_column feature used for survival modeling along with actual target as event.]
ExpandingMean.py [CatBoost-style target encoding. See  for short explanation]
leaky_mean_target_encoder.py [Example implementation of a out-of-fold target encoder (leaky, not recommended)]
auto_arima_forecast.py [Auto ARIMA transformer is a time series transformer that predicts target using ARIMA models.]
general_time_series_transformer.py [Demonstrates the API for custom time-series transformers.]
parallel_auto_arima_forecast.py [Parallel Auto ARIMA transformer is a time series transformer that predicts target using ARIMA models.In this implementation, Time Group Models are fitted in parallel]
parallel_prophet_forecast.py [Parallel FB Prophet transformer is a time series transformer that predicts target using FBProphet models.]
parallel_prophet_forecast_using_individual_groups.py [Parallel FB Prophet transformer is a time series transformer that predicts target using FBProphet models.This transformer fits one model for each time group column values and is significantly fasterthan the implementation available in parallel_prophet_forecast.py.]
serial_prophet_forecast.py [Transformer that uses FB Prophet for time series prediction.Please see the parallel implementation for more information]
time_encoder_transformer.py [converts the Time Column to an ordered integer]
trading_volatility.py [Calculates Historical Volatility for numeric features (makes assumptions on the data)]
Recipes for H2O Driverless AI
About Driverless AI
About BYOR
What are Custom Recipes?
Best Practices for Recipes
Reference Guide
Sample Recipes
Security
Safety
Performance
Count: 210
